"""
è®­ç»ƒç»“æœå½’æ¡£è„šæœ¬

åŠŸèƒ½:
1. è¯»å–æœ€è¿‘çš„checkpoint
2. ä¿å­˜è®­ç»ƒé…ç½®å’ŒLossä¿¡æ¯
3. å½’æ¡£å¯è§†åŒ–å›¾åƒ
4. ä»¥æ—¶é—´æˆ³å‘½åå­˜æ¡£æ–‡ä»¶å¤¹
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import shutil
from datetime import datetime
from pathlib import Path
import torch


def get_latest_checkpoint(checkpoint_dir='checkpoints'):
    """è·å–æœ€æ–°çš„checkpointæ–‡ä»¶"""
    ckpt_path = Path(checkpoint_dir)
    if not ckpt_path.exists():
        raise FileNotFoundError(f"Checkpoint directory '{checkpoint_dir}' not found!")
    
    # æŸ¥æ‰¾æ‰€æœ‰.pthæ–‡ä»¶
    ckpt_files = list(ckpt_path.glob('*.pth'))
    if not ckpt_files:
        raise FileNotFoundError(f"No checkpoint files found in '{checkpoint_dir}'!")
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
    latest_ckpt = max(ckpt_files, key=lambda p: p.stat().st_mtime)
    return latest_ckpt


def load_checkpoint_info(ckpt_path):
    """åŠ è½½checkpointä¿¡æ¯"""
    try:
        state_dict = torch.load(ckpt_path, map_location='cpu')
        
        # å°è¯•æå–å‚æ•°æ•°é‡
        n_params = sum(p.numel() for p in state_dict.values() if isinstance(p, torch.Tensor))
        
        info = {
            'checkpoint_path': str(ckpt_path),
            'file_size_mb': ckpt_path.stat().st_size / (1024 * 1024),
            'modified_time': datetime.fromtimestamp(ckpt_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
            'n_parameters': n_params,
        }
        return info
    except Exception as e:
        print(f"Warning: Failed to load checkpoint details: {e}")
        return {'checkpoint_path': str(ckpt_path)}


def parse_train_config(checkpoint_dir='checkpoints'):
    """
    ä»ä¿å­˜çš„é…ç½®æ–‡ä»¶ä¸­è¯»å–è®­ç»ƒé…ç½®
    
    Args:
        checkpoint_dir: checkpointç›®å½•è·¯å¾„
    
    Returns:
        configå­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    config_path = Path(checkpoint_dir) / 'last_config.json'
    
    if not config_path.exists():
        print(f"Warning: Config file not found at {config_path}")
        print("         Using default placeholder config")
        # è¿”å›å ä½ç¬¦é…ç½®
        return {
            'note': 'Config file not found, using placeholder',
            'n_samples': 'N/A',
            'train_ratio': 'N/A',
        }
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"âœ“ Config loaded from: {config_path}")
        return config
    except Exception as e:
        print(f"Warning: Failed to load config: {e}")
        return {'error': str(e)}


def find_latest_images(image_dir='images'):
    """æŸ¥æ‰¾æœ€æ–°çš„å¯è§†åŒ–å›¾åƒ"""
    img_path = Path(image_dir)
    if not img_path.exists():
        print(f"Warning: Image directory '{image_dir}' not found!")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = []
    for ext in ['*.png', '*.jpg', '*.pdf']:
        image_files.extend(img_path.glob(ext))
    
    if not image_files:
        print(f"Warning: No image files found in '{image_dir}'!")
        return []
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    image_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    
    return image_files


def extract_best_loss(checkpoint_dir='checkpoints'):
    """
    ä»ä¿å­˜çš„æŒ‡æ ‡æ–‡ä»¶ä¸­æå–Lossä¿¡æ¯
    
    Args:
        checkpoint_dir: checkpointç›®å½•è·¯å¾„
    
    Returns:
        lossä¿¡æ¯å­—å…¸
    """
    metrics_path = Path(checkpoint_dir) / 'last_metrics.pth'
    
    if not metrics_path.exists():
        print(f"Warning: Metrics file not found at {metrics_path}")
        return {
            'best_train_loss': 'N/A',
            'best_test_loss': 'N/A',
            'final_train_loss': 'N/A',
            'final_test_loss': 'N/A',
            'n_epochs': 'N/A',
        }
    
    try:
        metrics = torch.load(metrics_path, map_location='cpu')
        
        loss_info = {
            'best_test_loss': f"{metrics.get('best_test_loss', 'N/A'):.6f}" if isinstance(metrics.get('best_test_loss'), float) else 'N/A',
            'final_train_loss': f"{metrics.get('final_train_loss', 'N/A'):.6f}" if isinstance(metrics.get('final_train_loss'), float) else 'N/A',
            'final_test_loss': f"{metrics.get('final_test_loss', 'N/A'):.6f}" if isinstance(metrics.get('final_test_loss'), float) else 'N/A',
            'n_epochs': metrics.get('n_epochs_completed', 'N/A'),
            'train_losses_length': len(metrics.get('train_losses', [])),
            'test_losses_length': len(metrics.get('test_losses', [])),
        }
        
        print(f"âœ“ Metrics loaded from: {metrics_path}")
        return loss_info
        
    except Exception as e:
        print(f"Warning: Failed to load metrics: {e}")
        return {'error': str(e)}


def create_archive(archive_dir='archives'):
    """åˆ›å»ºå½’æ¡£æ–‡ä»¶å¤¹"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    archive_path = Path(archive_dir) / f'training_{timestamp}'
    archive_path.mkdir(parents=True, exist_ok=True)
    
    return archive_path, timestamp


def generate_report(archive_path, config, ckpt_info, loss_info, image_files):
    """ç”ŸæˆMarkdownæ ¼å¼çš„è®­ç»ƒæŠ¥å‘Š"""
    report_lines = []
    report_lines.append(f"# Training Results Report")
    report_lines.append(f"\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # 1. Checkpointä¿¡æ¯
    report_lines.append("## 1. Checkpoint Information\n")
    report_lines.append(f"- **File**: `{ckpt_info.get('checkpoint_path', 'N/A')}`")
    report_lines.append(f"- **Size**: {ckpt_info.get('file_size_mb', 0):.2f} MB")
    report_lines.append(f"- **Modified**: {ckpt_info.get('modified_time', 'N/A')}")
    report_lines.append(f"- **Parameters**: {ckpt_info.get('n_parameters', 'N/A'):,}\n")
    
    # 2. è®­ç»ƒé…ç½®
    report_lines.append("## 2. Training Configuration\n")
    report_lines.append("```json")
    report_lines.append(json.dumps(config, indent=2))
    report_lines.append("```\n")
    
    # 3. Lossä¿¡æ¯
    report_lines.append("## 3. Training Metrics\n")
    report_lines.append(f"- **Epochs Completed**: {loss_info.get('n_epochs', 'N/A')}")
    report_lines.append(f"- **Best Test Loss**: {loss_info.get('best_test_loss', 'N/A')}")
    report_lines.append(f"- **Final Train Loss**: {loss_info.get('final_train_loss', 'N/A')}")
    report_lines.append(f"- **Final Test Loss**: {loss_info.get('final_test_loss', 'N/A')}\n")
    
    # 4. å¯è§†åŒ–å›¾åƒ
    report_lines.append("## 4. Visualizations\n")
    if image_files:
        for img in image_files[:5]:  # æœ€å¤šæ˜¾ç¤º5å¼ 
            report_lines.append(f"- `{img.name}`")
    else:
        report_lines.append("- No visualization images found")
    
    report_lines.append("\n---\n*Generated by ONet_UDE Archive Script*")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = archive_path / 'README.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"âœ“ Report saved to: {report_path}")
    return report_path


def archive_results():
    """ä¸»å½’æ¡£æµç¨‹"""
    print("=" * 70)
    print("Training Results Archive Script")
    print("=" * 70)
    
    try:
        # 1. è·å–æœ€æ–°checkpoint
        print("\n[1/6] Finding latest checkpoint...")
        ckpt_path = get_latest_checkpoint()
        print(f"âœ“ Found: {ckpt_path}")
        
        # 2. åŠ è½½checkpointä¿¡æ¯
        print("\n[2/6] Loading checkpoint information...")
        ckpt_info = load_checkpoint_info(ckpt_path)
        print(f"âœ“ Checkpoint size: {ckpt_info.get('file_size_mb', 0):.2f} MB")
        
        # 3. è¯»å–é…ç½® - ã€ä¿®æ”¹ã€‘ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–
        print("\n[3/6] Loading training configuration...")
        config = parse_train_config()
        print(f"âœ“ Config loaded: {len(config)} parameters")
        
        # 4. æå–Lossä¿¡æ¯ - ã€ä¿®æ”¹ã€‘ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–
        print("\n[4/6] Extracting loss information...")
        loss_info = extract_best_loss()
        print(f"âœ“ Best test loss: {loss_info.get('best_test_loss', 'N/A')}")
        
        # 5. æŸ¥æ‰¾å¯è§†åŒ–å›¾åƒ
        print("\n[5/6] Finding visualization images...")
        image_files = find_latest_images()
        print(f"âœ“ Found {len(image_files)} image(s)")
        
        # 6. åˆ›å»ºå½’æ¡£
        print("\n[6/6] Creating archive...")
        archive_path, timestamp = create_archive()
        
        # ä¿å­˜é…ç½®JSON
        config_path = archive_path / 'config.json'
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        print(f"âœ“ Config saved: {config_path}")
        
        # ä¿å­˜checkpointä¿¡æ¯JSON
        ckpt_info_path = archive_path / 'checkpoint_info.json'
        with open(ckpt_info_path, 'w') as f:
            json.dump(ckpt_info, f, indent=2)
        print(f"âœ“ Checkpoint info saved: {ckpt_info_path}")
        
        # ä¿å­˜Lossä¿¡æ¯JSON
        loss_path = archive_path / 'loss_metrics.json'
        with open(loss_path, 'w') as f:
            json.dump(loss_info, f, indent=2)
        print(f"âœ“ Loss metrics saved: {loss_path}")
        
        # å¤åˆ¶checkpoint
        ckpt_archive_path = archive_path / ckpt_path.name
        shutil.copy2(ckpt_path, ckpt_archive_path)
        print(f"âœ“ Checkpoint copied: {ckpt_archive_path}")
        
        # ã€æ–°å¢ã€‘å¤åˆ¶åŸå§‹é…ç½®å’ŒæŒ‡æ ‡æ–‡ä»¶
        src_config = Path('checkpoints') / 'last_config.json'
        src_metrics = Path('checkpoints') / 'last_metrics.pth'
        
        if src_config.exists():
            shutil.copy2(src_config, archive_path / 'last_config.json')
            print(f"âœ“ Original config copied")
        
        if src_metrics.exists():
            shutil.copy2(src_metrics, archive_path / 'last_metrics.pth')
            print(f"âœ“ Original metrics copied")
        
        # å¤åˆ¶å¯è§†åŒ–å›¾åƒ
        if image_files:
            img_archive_dir = archive_path / 'images'
            img_archive_dir.mkdir(exist_ok=True)
            for img in image_files[:10]:  # æœ€å¤šå¤åˆ¶10å¼ æœ€æ–°çš„
                shutil.copy2(img, img_archive_dir / img.name)
            print(f"âœ“ Images copied: {len(image_files[:10])} files")
        
        # ç”ŸæˆæŠ¥å‘Š
        generate_report(archive_path, config, ckpt_info, loss_info, image_files)
        
        # å®Œæˆ
        print("\n" + "=" * 70)
        print(f"âœ… Archive created successfully!")
        print(f"ğŸ“ Location: {archive_path.absolute()}")
        print("=" * 70)
        print("\nArchive contents:")
        print(f"  - config.json           (è®­ç»ƒé…ç½®)")
        print(f"  - checkpoint_info.json  (æ¨¡å‹ä¿¡æ¯)")
        print(f"  - loss_metrics.json     (LossæŒ‡æ ‡)")
        print(f"  - {ckpt_path.name}      (æ¨¡å‹æƒé‡)")
        print(f"  - images/               (å¯è§†åŒ–å›¾åƒ)")
        print(f"  - README.md             (è®­ç»ƒæŠ¥å‘Š)")
        
        return archive_path
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    archive_results()
